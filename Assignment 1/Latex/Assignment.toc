\contentsline {section}{(Part 1) Learning Basics of Regression in Python}{1}{section*.2}
\contentsline {subsection}{Describe and Summarize the data}{1}{section*.3}
\contentsline {subsection}{Visualization}{2}{section*.4}
\contentsline {subsection}{Feature Weights}{3}{section*.5}
\contentsline {subsection}{Mean Squared Error Value}{3}{section*.6}
\contentsline {subsection}{Two Other Loss Functions}{3}{section*.7}
\contentsline {subsection}{Most Significant Features}{4}{section*.8}
\contentsline {section}{(Part 2) Locally Reweighted Regression}{5}{section*.9}
\contentsline {subsection}{(1) Solution to the Weighted Least Square Problem}{5}{section*.10}
\contentsline {subsection}{(3) k-fold Cross Validation Plots}{7}{section*.11}
\contentsline {subsection}{(4) Algorithm Behaviour for $\tau \to 0$ and $\tau \to \infty $}{8}{section*.12}
\contentsline {section}{(Part 3) Mini-batch SGD Gradient Estimator}{8}{section*.13}
\contentsline {subsection}{(1) Mini-Batch Proof}{8}{section*.14}
\contentsline {subsection}{(2) Show $\mathbb {E}_{I}[\nabla L_{I}(x,y,\theta )] = \nabla L(x,y,\theta )$}{9}{section*.15}
\contentsline {subsection}{(3) Importance of the Previous Result}{10}{section*.16}
\contentsline {subsection}{(4a) $\nabla L$ for Cost Function}{10}{section*.17}
\contentsline {subsection}{(5) True Gradient ($\nabla L$)vs. Batch Gradient Comparisons}{11}{section*.18}
\contentsline {subsection}{(6) Plot $\qopname \relax o{log}{\sigma _{ Ìƒj}}$ against $\qopname \relax o{log}m$}{12}{section*.19}
